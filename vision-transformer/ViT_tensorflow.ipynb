{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fb1801-aa36-44bc-9d16-ce15d4d4853e",
   "metadata": {},
   "source": [
    "# Vision transformer in Tensorflow (Unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc2ac21-0328-422b-8135-719a26fd7060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d99a1-7485-4838-bfb4-b7d6c62bef67",
   "metadata": {},
   "source": [
    "Based on https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7186a0d6-ba69-42af-8f68-3b8786f604e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42cb332a-499f-4565-9c33-7b7d6cba1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers as ly\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2c272c-3359-43cb-9919-d09028755ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "aug_image_size= 64\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        ly.Normalization(),\n",
    "        ly.Resizing(aug_image_size, aug_image_size),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "# data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45002451-8572-4c34-b5b8-d91509b00746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim, mlp_dim, dropout=0.1):\n",
    "    model = keras.Sequential([\n",
    "        ly.Dense(mlp_dim, activation=tf.nn.gelu),\n",
    "        ly.Dropout(dropout),\n",
    "        ly.Dense(dim),\n",
    "        ly.Dropout(dropout)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4501e48b-c30a-410c-8962-0482cc67897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(tf.keras.layers.Layer):\n",
    "    def __init__(self, name):\n",
    "        super(Identity, self).__init__(name=name)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab0e72f-dc42-497e-87f5-47a392f3c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(ly.Layer):\n",
    "    def __init__(self, dim, mlp_dim, heads = 8, dropout = 0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.lnorm1 = ly.LayerNormalization(epsilon=1e-6)\n",
    "        self.attn = ly.MultiHeadAttention(num_heads=heads, key_dim=dim, dropout=dropout)\n",
    "        self.lnorm2 = ly.LayerNormalization(epsilon=1e-6)\n",
    "        self.ff = feed_forward(dim, mlp_dim, dropout=dropout)\n",
    "    def call(self, x):\n",
    "        x1 = self.lnorm1(x)\n",
    "        attn = self.attn(x1, x1, x1)\n",
    "        x2 = x + attn\n",
    "        x3 = self.lnorm2(x2)\n",
    "        x3 = self.ff(x3)\n",
    "        x4 = x3 + x2\n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e592bdda-3be4-4d5e-a2cf-334b9cf49214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(ly.Layer):\n",
    "    def __init__(self, patch_size, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = ly.Dense(units=projection_dim)\n",
    "        self.position_embedding = ly.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        \n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd0ceb1b-f7b8-47aa-9aaa-c48bd66a4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisionTransformer(image_size=224, patch_size=16, num_classes=1000,\n",
    "                     dim=192, depth=12, heads=8, mlp_dim=768, channels = 3, \n",
    "                      dim_head = 64, dropout = 0.1, emb_dropout = 0.1):\n",
    "    \n",
    "    inputs = keras.Input(shape=(image_size, image_size, 3))\n",
    "    augmented = data_augmentation(inputs)\n",
    "    im_h, im_w = augmented.shape[1:3]\n",
    "    patch_h, patch_w = patch_size, patch_size\n",
    "    num_patches = (im_h // patch_h) * (im_w // patch_w)\n",
    "    patch_dim = channels * patch_h * patch_w\n",
    "    \n",
    "    patch_encoded = PatchEncoder(patch_size, num_patches, patch_dim)(augmented)\n",
    "    patch_encoded = ly.Dense(dim)(patch_encoded)\n",
    "    patch_encoded = ly.Dropout(emb_dropout)(patch_encoded)\n",
    "\n",
    "    for _ in range(depth):\n",
    "        patch_encoded = TransformerBlock(dim, mlp_dim, dropout=dropout)(patch_encoded)\n",
    "    \n",
    "    representation = ly.LayerNormalization(epsilon=1e-6)(patch_encoded)\n",
    "    representation = ly.Flatten()(representation)\n",
    "    \n",
    "    out = ly.Dense(num_classes)(representation)\n",
    "    return keras.Model(inputs=inputs, outputs=out, name=\"vision_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7082a64-9b59-4ba5-8f7a-b0daf9be60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224 # standard image size\n",
    "channels = 3 # RGB image\n",
    "patch_size = 16\n",
    "embed_dim = 192\n",
    "mlp_dim = embed_dim * 4\n",
    "depth = 12\n",
    "n_heads = 3\n",
    "dropout = 0.1\n",
    "emb_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1839ae0-b069-461d-a4a9-146da49c0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(image_size=image_size, \n",
    "                          patch_size=patch_size, num_classes=1000, \n",
    "                          dim=embed_dim, mlp_dim=mlp_dim, \n",
    "                          dropout=dropout, emb_dropout=emb_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "143bf59d-bd60-4cd7-8716-a2354dae9737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.random.rand(1, 224, 224, 3)\n",
    "model(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7156eb37-2f27-4328-a543-72c6042155c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 64, 64, 3)        7         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " patch_encoder (PatchEncoder  (None, 16, 768)          602880    \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16, 192)           147648    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 192)           0         \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 16, 192)          1481088   \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_3 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_4 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_5 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_6 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_8 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_9 (Transf  (None, 16, 192)          1481088   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_10 (Trans  (None, 16, 192)          1481088   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_11 (Trans  (None, 16, 192)          1481088   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " layer_normalization_24 (Lay  (None, 16, 192)          384       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1000)              3073000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,596,975\n",
      "Trainable params: 21,596,968\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a1844-c5ef-403e-823c-4222e2beca99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
