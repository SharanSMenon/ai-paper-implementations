{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfd6788-6350-41f3-84e6-c72864894e0b",
   "metadata": {},
   "source": [
    "# Vision Transformer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93862b7-5af0-41bf-926b-b74474c11a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Reduce, Rearrange\n",
    "import torchinfo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a228d4-f273-4458-a636-1d0e85a0755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224 # standard image size\n",
    "channels = 3 # RGB image\n",
    "patch_size = 16\n",
    "embed_dim = 192\n",
    "mlp_dim = embed_dim * 4\n",
    "depth = 12\n",
    "n_heads = 3\n",
    "dropout = 0.1\n",
    "emb_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bc0551-7382-429b-8e8b-6d49556130cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, dim, patch_dim, patch_height, patch_width, num_patches, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ff(x)\n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c2275b-bcb9-48fd-b9ce-b15063c472b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.scale = embed_dim ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(embed_dim, embed_dim * 3, bias = False)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ddcd23-b06b-44fd-9b74-a86645b67246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, dim_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        embed_dim: embed dimension\n",
    "        \"\"\"\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout)\n",
    "        self.attn = MultiHeadAttention(embed_dim, heads, dropout=dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "                nn.Linear(embed_dim, dim_ff),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(dim_ff, embed_dim),\n",
    "                nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm1(x)\n",
    "#         x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.attn(inp_x)\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee8d79f-0d04-4cf6-873a-4038aeb8fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    dim: embed dimension\n",
    "    mlp_dim: feedforward network out dim\n",
    "    heads: number of heads\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, depth, heads, mlp_dim, dropout = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            layers.append(\n",
    "                TransformerLayer(embed_dim, heads, mlp_dim, dropout=dropout)\n",
    "            )\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd714c63-7aac-456f-b724-1e35e759f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=224, \n",
    "                 patch_size=16, num_classes=1000,\n",
    "                 dim=192, depth=12, heads=4, \n",
    "                 mlp_dim=768, channels = 3, \n",
    "                 dropout = 0.1, emb_dropout = 0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        im_h, im_w = image_size, image_size\n",
    "        patch_h, patch_w = patch_size, patch_size\n",
    "        self.num_patches = (im_h // patch_h) * (im_w // patch_w)\n",
    "        self.patch_dim = channels * patch_h * patch_w\n",
    "        \n",
    "        self.to_patch_embedding = PatchEmbedding(dim, self.patch_dim, patch_h, patch_w, self.num_patches,\n",
    "                                                 dropout = emb_dropout)\n",
    "        \n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout=dropout)\n",
    "\n",
    "        self.lnorm = nn.Sequential(\n",
    "            nn.Identity(),\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(dim, num_classes)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embedding(x)\n",
    "        # x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = self.lnorm(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42ee8a5-fc2c-414f-86a5-01d64f0f2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(\n",
    "    image_size = image_size,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = 1000,\n",
    "    dim = embed_dim,\n",
    "    depth = depth,\n",
    "    heads = n_heads,\n",
    "    mlp_dim = mlp_dim,\n",
    "    dropout = dropout,\n",
    "    emb_dropout = emb_dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b575e0-8309-42ae-ba56-55bc3d4f5781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1, channels, image_size, image_size)\n",
    "model(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc1fb50-591d-4867-ab74-48a628ef483d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             --                        --\n",
       "├─PatchEmbedding: 1-1                         [1, 197, 192]             --\n",
       "│    └─Sequential: 2-1                        [1, 196, 192]             --\n",
       "│    │    └─Rearrange: 3-1                    [1, 196, 768]             --\n",
       "│    │    └─Linear: 3-2                       [1, 196, 192]             147,648\n",
       "│    └─Dropout: 2-2                           [1, 197, 192]             --\n",
       "├─Transformer: 1-2                            [1, 197, 192]             --\n",
       "│    └─Sequential: 2-3                        [1, 197, 192]             --\n",
       "│    │    └─TransformerLayer: 3-3             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-4             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-5             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-6             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-7             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-8             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-9             [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-10            [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-11            [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-12            [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-13            [1, 197, 192]             444,288\n",
       "│    │    └─TransformerLayer: 3-14            [1, 197, 192]             444,288\n",
       "├─Sequential: 1-3                             [1, 192]                  --\n",
       "│    └─Identity: 2-4                          [1, 197, 192]             --\n",
       "│    └─Reduce: 2-5                            [1, 192]                  --\n",
       "│    └─LayerNorm: 2-6                         [1, 192]                  384\n",
       "├─Sequential: 1-4                             [1, 1000]                 --\n",
       "│    └─Linear: 2-7                            [1, 1000]                 193,000\n",
       "===============================================================================================\n",
       "Total params: 5,672,488\n",
       "Trainable params: 5,672,488\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 5.67\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 40.25\n",
       "Params size (MB): 22.69\n",
       "Estimated Total Size (MB): 63.54\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, (1, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498ceef8-76c5-45bb-9cae-92bf9ce696c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "510e8b96-aa05-47ac-8fd8-a612db149574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 22.893161\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
